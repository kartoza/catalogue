

% Do not remove the above two blank lines

== Setup Procedure for Fibrecat SX60 Storage arrays ==

=== Hardware preparation ===

As root / sudo edit the /etc/fstab and disable the storage array at boot. To do
this change this line:

```
UUID=3bdda6ae-3195-47b0-955e-278b5ed51da5 /data    ext3       auto,nouser,noexec,nosuid,rw        1 2
```

to look like this:

```
#UUID=3bdda6ae-3195-47b0-955e-278b5ed51da5 /data    ext3       auto,nouser,noexec,nosuid,rw        1 2
```

The host system was powered off:

```
sudo /sbin/halt
```

The fibrecat system was powered off (note that it has redundant power supplies
and both must be powered off).

All drives were then removed and their relative positions and serial numbers
were recorded.

The drives were then removed from their caddies and replacement 1.5TB drives
were inserted in their place.

After all the replacement drives were inserted, the storage array was powered
up again and the system restarted.

We checked that all drives came up properly and no warning lights were displayed.

=== Configure the storage array in the web admin interface ===

The array is configured using a web control panel at 

```
http://196.35.94.141
```

To login use _____________ for username and _________________ for password.



Next we had to create a virtual disk - removing the old drives destroys any pre-existing vdisks.

- Click on the **manage** link  on the left and then choose **create a vdisk**
- Next choose **Manual Virtual Disk Creation**
- Enter 'sarmesstorage' in the **Enter Virtual Disk Name** box
- For **Select Virtual Disk RAID Level** choose 'Raid 5 - Parity RAID, Parity Distributed'
- Click next to proceed onto disk creation
- Tick all the drives in the enclosure diagram except for the last
- Calculate Formatted Virtual Disk Size of Selected Drives - click this button and verify output is something like that shown below here ```For a RAID Level 5, your selected drives will approximately yield a 15.00 TByte final virtual disk capacity.```
- For the tickbox **Would you like to add dedicated spare drives for this virtual disk?**, choose Yes
- Click continue
- All the drive bays save the last should now be shown in blue.
- Tick the remaining green bay and then click 'continue' next to the **Add Selected Dedicated Spare Drives to "sarmesstorage" and Continue Creating Virtual Disk:** prompt.
- 

At this stage you will be shown a report that should look something like this:


```
Virtual Disk Name:  sarmesstorage
RAID Level:     5
Virtual Disk Size:  201644.88 GBytes
Drives Chosen:  
Serial Number   WWN     Size (GBytes)   Encl.Slot
9VS1J496    5000C50011343DE3    1500.30     0.0
9VS1E8SL    5000C50011229530    1500.30     0.1
9VS1BWB5    5000C5001115F067    1500.30     0.2
9VS1BM5F    5000C5001111301C    1500.30     0.3
9VS1E0WR    5000C500111FC913    1500.30     0.4
9VS1HZ5M    5000C500113537B4    1500.30     0.5
9VS1FWXT    5000C500112BCEA2    1500.30     0.6
9VS1GH9V    5000C500113036EE    1500.30     0.7
9VS1GZMH    5000C5001130D35D    1500.30     0.8
9VS1H6A0    5000C50011353891    1500.30     0.9
9VS1FY6B    5000C500112C210C    1500.30     0.10
Dedicated Spare Drives Chosen:  
Serial Number   WWN     Size (GBytes)
9VS1H76L    5000C50011353C6C    1500.30
Virtual Disk Initialization:    Online
```

Now we can proceed to set up partitions ('Volumes') on the virtual disk.

```
Configure Volumes for Virtual Disk sarmesstorage
How Many Volumes    : 1
Create Volumes of Equal Size?   
Yes  
Expose Volumes to All Hosts?    
No
Automatically Assign LUNs?  
Disabled
Would You Like to Name Your Volumes?    
No
Advanced Virtual Disk Creation Options  Advanced Options - not used
```

Click 'Create virtual disk' A progress page will appear. Note that the process
will take a loooooong time!

**Note:** It took 3 or 4 days to build the virtual device with 1.5TB disks.

After the virtual disk is built, you need to create a volume mapping.

The volume mapping associates a fibre channel LUN connector to the volume.

In the managment web UI, click: Manage -> volume mapping -> map hosts to volumre.

For the sarmes machine we used the following configuration:


```
Current Host-Volume Relationships


WWN                 Host Name       LUN     Port 0 Access   Port 1 Access
10000000C96DABE6    Sarmes1_Port0    0       rw               rw
10000000C961BB34    Sarmes1_Port1    1       rw               rw
All Other Hosts                      None    none             none 

```

**Note** You probably only need to map one WWN / Host / Lun - we think you only need 
to map Sarmes1_port1 to 10000000C961BB34 but you will need to test experimentally to 
be sure.

After making these config changes, reboot the sarmes server.

Watch the boot messages or check 

```
dmesg
```

You should see a new device listed like this:

```
sd 1:0:0:0: [sdc] Very big device. Trying to use READ CAPACITY(16).
sd 1:0:0:0: [sdc] 29302441984 512-byte hardware sectors (15002850 MB)
sd 1:0:0:0: [sdc] Write Protect is off
sd 1:0:0:0: [sdc] Mode Sense: 93 00 00 08
sd 1:0:0:0: [sdc] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA
sd 1:0:0:0: [sdc] Very big device. Trying to use READ CAPACITY(16).
sd 1:0:0:0: [sdc] 29302441984 512-byte hardware sectors (15002850 MB)
sd 1:0:0:0: [sdc] Write Protect is off
sd 1:0:0:0: [sdc] Mode Sense: 93 00 00 08
sd 1:0:0:0: [sdc] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA
sdc: unknown partition table
sd 1:0:0:0: [sdc] Attached SCSI disk
```

You can see the drive came up as sdc. It pushes the previous sdc drive down to 
sdd. This is not a problem though since the /etc/fstab uses UUIDs to reference partitions.


Next you can verify this using fdisk:

```
sudo /sbin/fdisk -l /dev/sdc 

Disk /dev/sdc: 15002.8 GB, 15002850295808 bytes
255 heads, 63 sectors/track, 1823992 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes
Disk identifier: 0x00000000

Disk /dev/sdc doesn't contain a valid partition table
```


By default a DOS partition table is used on new devices and by fdisk. One major
limitation of this is that it does not support partition sizes greater than
2TB, meaning that most of your large disk device will be inaccessible!

There are two ways two resolve this - using a raw xfs partition (as we have
done on SARMES), or using the GPT partition table scheme as we have done on
LION).

=== Partitioning option 1:  Raw xfs filesystem ===


Lets first look at / check the partition table on this device:

```
sudo /sbin/fdisk /dev/sdc
```

```
sudo /sbin/fdisk /dev/sdc
Device contains neither a valid DOS partition table, nor Sun, SGI or OSF disklabel
Building a new DOS disklabel with disk identifier 0x5ae412a5.
Changes will remain in memory only, until you decide to write them.
After that, of course, the previous content won't be recoverable.


The number of cylinders for this disk is set to 1823992.
There is nothing wrong with that, but this is larger than 1024,
and could in certain setups cause problems with:
1) software that runs at boot time (e.g., old versions of LILO)
2) booting and partitioning software from other OSs
   (e.g., DOS FDISK, OS/2 FDISK)
Warning: invalid flag 0x0000 of partition table 4 will be corrected by w(rite)

Command (m for help): p

Disk /dev/sdc: 15002.8 GB, 15002850295808 bytes
255 heads, 63 sectors/track, 1823992 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes
Disk identifier: 0x5ae412a5

Device Boot      Start         End      Blocks   Id  System

Command (m for help): n
Command action
e   extended
p   primary partition (1-4)
p
Partition number (1-4): 1
First cylinder (1-1823992, default 1): 
Using default value 1
Last cylinder or +size or +sizeM or +sizeK (1-267349, default 267349): 
Using default value 267349

Command (m for help): w
The partition table has been altered!

Calling ioctl() to re-read partition table.
Syncing disks.
```

**Note:** We have left the disk unpartitioned. Doing this is required 
because when partitioning we found that a max of 2TB were allocated. 
Since xfs supports using a raw unpartitioned disk, we are taking this route.


Now we have one BIG 15TB partition but no filesystem yet. We 
will format the partition with xfs:

```
sudo /sbin/mkfs.xfs -L "xfsdata" /dev/sdc
```

After its done it will show some statistics.

```
/sbin/mkfs.xfs -L "xfsdata" -f /dev/sdc
meta-data=/dev/sdc               isize=256    agcount=14, agsize=268435455 blks
         =                       sectsz=512   attr=2
data     =                       bsize=4096   blocks=3662805248, imaxpct=5
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096  
log      =internal log           bsize=4096   blocks=32768, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=0
realtime =none                   extsz=4096   blocks=0, rtextents=0
```

Now find out its UUID:

```
sudo /sbin/blkid
```

Now add an entry to /etc/fstab:

```
/dev/sdc /data2 xfs auto,noexec,nosuid,nouser,rw 1 2
```

**Note:** We are not using a UUID for this device since SUSE seems to 
not be able to find xfs non partitioned disks at boot up by their UUID!!!

At time of writing this, the complete fstab looked like this (note lines
starting with # are comments):

```
#swap 1
UUID=d5b38bbe-7544-44d4-94cd-e34d2e96c7c4 none            swap    sw              0       0
#swap 2
UUID=d076d65e-9de1-4bd8-a83f-a86f966d6642 none            swap    sw              0       0
#UUID=                swap                 swap       defaults              0 0
UUID=3829d9de-ae4d-4d37-8dac-6ff9adbb8da4 /                    ext3       acl,user_xattr        1 1
UUID=8b9e0e92-a923-4918-8344-1582819f3480 /tmp                 ext3       acl,user_xattr        1 2
proc                 /proc                proc       defaults              0 0
sysfs                /sys                 sysfs      noauto                0 0
debugfs              /sys/kernel/debug    debugfs    noauto                0 0
usbfs                /proc/bus/usb        usbfs      noauto                0 0
devpts               /dev/pts             devpts     mode=0620,gid=5       0 0
# Note we wanted to use umask here but xfs does not seem to support it!
##UUID=bb5379a1-91e6-4345-9592-4214a74c8e12 /data xfs auto,noexec,nosuid,nouser,rw 1 2
#UUID=3bdda6ae-3195-47b0-955e-278b5ed51da5 /data    ext3       auto,nouser,noexec,nosuid,rw        1 2
/dev/sdc /data2 xfs auto,noexec,nosuid,nouser,rw 1 2
# Old data2 storage array:
#UUID=a9154619-2a3d-4d9f-8316-047dc539550e /data2 xfs auto,noexec,nosuid,nouser,rw 1 2
196.35.94.195:/mnt/data1/EOSC/EOSC      /IRMES  nfs     auto,nouser,noexec,nosuid,rw 0 0
#smb://eouser@sac-msd01/eosc/IRMES
#//196.35.94.195/eosc/ /IRMES cifs user,credentials=/etc/samba_credentials,uid=eouser,gid=EOSC,file_mode=0770,dir_mode=0770 0 0

# output from /sbin/blkid (do not uncomment any of these lines):
#/dev/sda1: TYPE="swap" LABEL="swap1" UUID="d5b38bbe-7544-44d4-94cd-e34d2e96c7c4" 
#/dev/sda2: LABEL="slash" UUID="3829d9de-ae4d-4d37-8dac-6ff9adbb8da4" TYPE="ext3" 
#/dev/sdb1: TYPE="swap" LABEL="swap2" UUID="d076d65e-9de1-4bd8-a83f-a86f966d6642" 
#/dev/sdb2: LABEL="tmp" UUID="8b9e0e92-a923-4918-8344-1582819f3480" TYPE="ext3" 
#/dev/dm-0: UUID="a9154619-2a3d-4d9f-8316-047dc539550e" TYPE="xfs" 
#/dev/sdd: UUID="Vg1MI3-GHBJ-mcyI-BNRn-c5nP-MbgJ-pPWKvI" TYPE="lvm2pv" 
#/dev/sdc: UUID="" TYPE="ext3" SEC_TYPE="ext2"
```

Now reboot the system and the sdc should be activated. In the example above we
retured the drive it replaced by commenting it out. We plan to remove /dev/dm-0 after 
the upgrade is done.


=== Partitioning option 2:  Creating a large filesystem using GPT ===

For newer systems we want to use ext4 on a large (non raw) filesystem The
kernel must have been compiled with GPT support (it is by default under UBUNTU
Jaunty Server Edition >= 9.04). In addition, we need to use **parted** (the
command line version of gparted) to format the disk and create the GPT
partition table.

In Linux parlance, determining the partition table type is called 'setting the
disk label'. In the console transcripts that follow we will set the disk label
to GPT, create a large single partition and then format and mount the drive.
Once this has been completed, we will use a similar procedure as described
above to add an fstab entry so that the volume is mounted at boot time.


This is the procedure I used to create a large ext4 partition using parted:

```
(parted) unit s
(parted) print
Model: FSC FibreCAT_SX1 (scsi)
Disk /dev/sdc: 29302441984s
Sector size (logical/physical): 512B/512B
Partition Table: gpt

Number  Start  End  Size  File system  Name  Flags

(parted) mkpart
Partition name?  []? cataloguestorage2
File system type?  [ext2]? ext4
Start? 34
End? 29302441950
Warning: The resulting partition is not properly aligned for best performance.
Ignore/Cancel? cancel
(parted) mkpart cataloguestorage2 ext4 1 -1
Warning: You requested a partition from 1s to 29302441983s.
The closest location we can manage is 34s to 29302441950s.
Is this still acceptable to you?
Yes/No? yes
Warning: The resulting partition is not properly aligned for best performance.
Ignore/Cancel? Ignore
(parted) p
Model: FSC FibreCAT_SX1 (scsi)
Disk /dev/sdc: 29302441984s
Sector size (logical/physical): 512B/512B
Partition Table: gpt

Number  Start  End           Size          File system  Name               Flags
 1      34s    29302441950s  29302441917s               cataloguestorage2
```

The process sets the drive units to sectors, then creates a new partition
leaving 34sectors at the start of the drive.

Now exit parted and create the filesystem:

```
sudo mkfs.ext4 /dev/sdc1
```

Note it will take a little while to process. Finally add a new mount point for
the partition and mount it.

```
mkdir /mnt/cataloguestorage2
```

Add an entry to /etc/fstab:

```
/dev/sdc1 /mnt/cataloguestorage2            ext4    relatime,errors=remount-ro        0       2
```
